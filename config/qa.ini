[COMMON]
datasets_dir = data
dataset_name = wiki
wordvec_initialization = nonorthonormalized
wordvec_path = glove/glove.6B.300d.txt
batch_size = 64
epochs = 60
eval_dir = eval
embedding_trainable = 0    
network_type = qarnn
dropout_keep = 1
measurement_size  = 20
clean = 1
pooling = wide  
gpu= 0
l2_reg_lambda = 0.000001
learning_rate = 0.001
filter_sizes=1,2,3,5
num_filters= 64
hidden_size=100




            




            
